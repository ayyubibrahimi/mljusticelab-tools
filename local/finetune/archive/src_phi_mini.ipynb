{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft bitsandbytes accelerate huggingface-hub accelerate flash-attn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.distributed as dist\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login, notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from accelerate import Accelerator\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Environment Variable Configuration\n",
    "#\n",
    "# TOKENIZERS_PARALLELISM:\n",
    "# - Current setting: \"false\"\n",
    "# - Purpose: Controls parallel tokenization operations in the transformers library\n",
    "# - Effects:\n",
    "#   - When \"false\": Disables parallel tokenization to prevent deadlocks in multiprocessing\n",
    "#   - When \"true\": Enables parallel tokenization which can speed up processing but may cause\n",
    "#                  issues in certain environments (especially notebooks)\n",
    "# - Change impacts:\n",
    "#   - Setting to \"true\" might improve tokenization speed but could cause stability issues\n",
    "#   - Recommended to keep \"false\" for notebook environments or when using DataLoader\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# PYTORCH_CUDA_ALLOC_CONF:\n",
    "# - Current setting: 'max_split_size_mb:50'\n",
    "# - Purpose: Controls CUDA memory allocation behavior\n",
    "# - Parameters:\n",
    "#   - max_split_size_mb: Maximum size of a single CUDA memory block that can be split\n",
    "# - Effects:\n",
    "#   - Helps prevent memory fragmentation\n",
    "#   - Reduces the likelihood of out-of-memory errors during training\n",
    "# - Change impacts:\n",
    "#   - Increasing the value (e.g., to 100) allows larger contiguous memory blocks but may\n",
    "#     increase fragmentation\n",
    "#   - Decreasing the value reduces fragmentation but may impact performance\n",
    "#   - Values too small might cause allocation failures\n",
    "#   - Values too large might lead to memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'\n",
    "\n",
    "\n",
    "# log in if you're using a gated model\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add these helper functions at the top\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            with torch.cuda.device(f'cuda:{i}'):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Function to move model between devices\n",
    "def move_model(model, device):\n",
    "    clear_memory()\n",
    "    return model.to(device)\n",
    "\n",
    "def format_data(example):\n",
    "    \"\"\"Format the conversation data into the expected chat format.\"\"\"\n",
    "    try:\n",
    "        user_message = example['messages'][0]['content']\n",
    "        assistant_message = example['messages'][1]['content']\n",
    "        \n",
    "        # Simple format without special tokens\n",
    "        full_prompt = f\"{user_message}\\n{assistant_message}\"\n",
    "        \n",
    "        return {\n",
    "            \"full_prompt\": full_prompt,\n",
    "            \"ground_truth\": assistant_message\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting example: {e}\")\n",
    "        return None\n",
    "\n",
    "def collate_fn(examples):\n",
    "    \"\"\"Collate function similar to the working vision-language approach\"\"\"\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    \n",
    "    for example in examples:\n",
    "        input_ids.append(torch.tensor(example['input_ids']))\n",
    "        attention_mask.append(torch.tensor(example['attention_mask']))\n",
    "        labels.append(torch.tensor(example['labels']))\n",
    "    \n",
    "    # Pad sequences\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def process_dataset(dataset, tokenizer):\n",
    "    \"\"\"Process and tokenize the dataset\"\"\"\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples['full_prompt'],\n",
    "            truncation=True,\n",
    "            max_length=5000,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Create labels (copy input_ids)\n",
    "        labels = [ids.copy() for ids in tokenized['input_ids']]\n",
    "        \n",
    "        # Find the position where assistant response starts\n",
    "        for idx, text in enumerate(examples['full_prompt']):\n",
    "            try:\n",
    "                # Find where the assistant message starts\n",
    "                user_message_len = len(tokenizer(examples['full_prompt'][idx].split('\\n')[0])['input_ids'])\n",
    "                # Mask the user message part\n",
    "                labels[idx][:user_message_len] = [-100] * user_message_len\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example {idx}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'],\n",
    "            'attention_mask': tokenized['attention_mask'],\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    # Format dataset\n",
    "    formatted_dataset = dataset.map(format_data, remove_columns=dataset.column_names)\n",
    "    \n",
    "    # Remove None entries\n",
    "    formatted_dataset = formatted_dataset.filter(lambda x: x is not None)\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = formatted_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=formatted_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "# Get number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "base_model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    # r: Controls the rank of LoRA adapters\n",
    "    # - Increasing r (e.g., 32, 64) = More model capacity but higher VRAM usage and training time\n",
    "    # - Decreasing r (e.g., 4, 8) = Less VRAM but may underfit on complex tasks\n",
    "    # - Rule of thumb: Double r if seeing underfitting, halve if running out of memory\n",
    "    r=512,\n",
    "    \n",
    "    lora_alpha=256,\n",
    "    \n",
    "    # Target modules for adaptation\n",
    "    # - Removing layers = Faster training but might miss important adaptations\n",
    "    # - Start with attention layers (q,k,v,o) if VRAM limited\n",
    "    # - Add MLP layers (gate,up,down) if more capacity needed\n",
    "    # target_modules=[\n",
    "    #     \"q_proj\",\n",
    "    #     \"k_proj\",\n",
    "    #     \"v_proj\",\n",
    "    #     \"o_proj\",\n",
    "    #     \"gate_proj\",\n",
    "    #     \"up_proj\",\n",
    "    #     \"down_proj\",\n",
    "    # ],\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "    # target_modules=\n",
    "    #     \"all-linear\",\n",
    "\n",
    "    \n",
    "    # use_rslora: Rank-Stabilized scaling\n",
    "    # - True = Better stability for higher ranks but slightly more compute\n",
    "    # - False = Original LoRA scaling, might be unstable with high ranks\n",
    "    use_rslora=True,\n",
    "    \n",
    "    # modules_to_save: Full-rank modules to train\n",
    "    # - Adding more = Better adaptation but much higher memory cost\n",
    "    # - Removing all = Pure LoRA training, minimum memory usage\n",
    "    # - lm_head commonly included for vocabulary adaptation\n",
    "    modules_to_save=[\"lm_head\", \"embedding_tokens\"],\n",
    "    \n",
    "    # bias: Which bias terms to train\n",
    "    # - \"none\" = No bias training, minimum memory\n",
    "    # - \"all\" = Train all biases, better for distribution shifts\n",
    "    # - \"lora_only\" = Middle ground, only LoRA biases\n",
    "    bias=\"lora_only\",\n",
    "    \n",
    "    # lora_dropout: Regularization strength\n",
    "    # - Higher (e.g., 0.2, 0.3) = More regularization, good for small datasets\n",
    "    # - Lower (e.g., 0.05, 0.0) = Less regularization, better for large datasets\n",
    "    # - Zero = No dropout, maximum learning but might overfit\n",
    "    lora_dropout=0.1,\n",
    "    \n",
    "    # task_type: Configures model behavior\n",
    "    # - \"CAUSAL_LM\" = Standard autoregressive training\n",
    "    # - \"SEQ_2_SEQ_LM\" = For encoder-decoder models\n",
    "    # - \"SEQ_CLS\" = For sequence classification\n",
    "    # - Changing affects loss computation and forward pass behavior\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Training Arguments with distributed training settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi-v3\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,  # Actual batch size per GPU\n",
    "    gradient_accumulation_steps=1,   # Number of forward passes before backward\n",
    "    \n",
    "    # Learning rate ranges:\n",
    "    # - 1e-5: Very conservative, slower learning\n",
    "    # - 1e-4: Standard for many tasks\n",
    "    # - 2e-4: Aggressive, good with larger batches\n",
    "    # - 5e-4+: Very aggressive, risk of divergence\n",
    "    learning_rate=1e-4,\n",
    "    \n",
    "    # Precision options\n",
    "    bf16=True,    # BFloat16: Better numerical stability than Float16\n",
    "    fp16=False,   # Disabled when using BFloat16\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch\",  # default\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    # Warmup steps ranges:\n",
    "    # - 0-50: Minimal warmup, good for small datasets\n",
    "    # - 100-500: Standard range for most tasks\n",
    "    # - 1000+: Very gradual warmup, safer for large learning rates\n",
    "    warmup_steps=25,\n",
    "    \n",
    "    # Gradient clipping ranges:\n",
    "    # - 0.1-0.3: Very conservative, stable but slow\n",
    "    # - 0.5-1.0: Standard range\n",
    "    # - 1.0-3.0: Aggressive, faster but risk of instability\n",
    "    max_grad_norm=.5,\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine: Good default, smooth decay\n",
    "    \n",
    "    # Memory optimization flags\n",
    "    gradient_checkpointing=True,  # True saves memory but ~20% slower,\n",
    "    gradient_checkpointing_kwargs = {\"use_reentrant\": False}, # use reentrant checkpointing\n",
    "\n",
    ")\n",
    "\n",
    "# Load and process datasets\n",
    "train_file = \"data/input/training_data.jsonl\"\n",
    "eval_file = \"data/input/validation_data.jsonl\"\n",
    "\n",
    "train_dataset = load_dataset('json', data_files=train_file, split='train')\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "eval_dataset = load_dataset('json', data_files=eval_file, split='train')\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "eval_dataset = eval_dataset.select(range(20))\n",
    "\n",
    "# Process datasets\n",
    "train_processed = process_dataset(train_dataset, tokenizer)\n",
    "eval_processed = process_dataset(eval_dataset, tokenizer)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_processed,\n",
    "    eval_dataset=eval_processed,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Disable model caching during training\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training completed. Preparing to save...\")\n",
    "clear_memory()\n",
    "model.save_pretrained(training_args.output_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "document_text = r\"\"\"\n",
    "Summarize each event. Include the date, names of persons involved, and the details of the event. \n",
    "\n",
    "Below is the document you will review:\n",
    "\n",
    "H\\nTermination recommended; Resigned\\nTerminated prior to IA findings for failure.\\nACTION\\nOfficer Ornelas resigned on 12/27/2016\\ncollateral duty position.\\nAppeal in process.\\nTermination recommended; Resigned\\nOn 3/10/15, demotion to police officer;\\nprobation.\\nMosqueda resigned on 1/31/19 prior to\\nsuspension from hostage negotiator SWAT\\nNotice to Terminate served. Resigned\\n160-hour suspension, PDSA served\\nTerminated 3/18/15 for failure to pass\\nremoval from training officer position;\\nNotice to Terminate served 7/19/18;\\nResigned 10/14/16 prior to the findings.\\n9/15/14\\n2/24/16:\\n5/1/16.\\n10/10/2016.\\nto pass probation.\\nTermination recommended; Officer\\nprior to the completion of this case.\\n\\u20b2\\nMar 20 2015\\nMar 20 2015\\nFinding Dt\\nApr 04.2018\\nFeb 5 2019\\nSep 10 2014\\nDec 02.2015\\nOct 05 2016\\nF\\nFinding\\nSustained\\nSustained\\nSustained\\nSustained\\nSustained Jun 15 2016\\nSustained\\nSustained |Oct 20 2016\\nSustained.\\nSustained |Jan 25 2017\\nSustained Feb:02-2015\\nSustained\\nFalsification of Work-Related\\nAllegation\\nDishonesty\\nDishonesty; False Statements\\nDocuments; False Statements\\nDishonesty; Falsification of Work-\\nOn-Duty Sexual Relations\\nDocuments\\nRelated Documents\\nDestruction of Evidence\\nFalse Statements\\nFalsification of Work-Related\\\"\\nDishonesty\\n|On-Duty Sexual Relations\\nD\\nOfficer Marc Aguilar [1145]\\nOfficer Kevin Schindler [1260]\\nOfficer Hillary Bjorneboe [1226]\\nOfficer Travis Brewer [1132]\\nOfficer Jeremy Salcido [1273]\\nOfficer Doug Mansker [843]\\nDetective Damacio Diaz [854]\\nOfficer Manuel Ornelas [989]\\nDetective Justin Lewis [1015]\\nOfficer Enrique Mosqueda (1242) |Sexual Solicitation\\nSr. Officer Kyle Ursery [969):\\nC\\nOct 06 2017\\nOct 16 2014\\nOct 16 2014\\nFeb 25 2015\\nJun 06 2016\\nAug 02 2016\\nJan 09 2015\\nOct 13 2018\\nMay 31 2016\\nOct 05 2016\\nInc Received Dt Involved Officer\\nJun 24 2014\\nB\\nInternal\\nInternal\\nInternal\\nInternal\\nInternal\\nInternal\\nInternal\\nInternal\\nInternal\\nInternal\\nType\\nInternal\\nA\\nIA2015-006\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_models(base_model_id, adapter_path=None):\n",
    "    \"\"\"Load both base and fine-tuned models for comparison\"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    \n",
    "    # Load base model and create pipeline\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\"\n",
    "    )\n",
    "    base_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Load fine-tuned model\n",
    "    if adapter_path:\n",
    "        fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "        fine_tuned_model = PeftModel.from_pretrained(fine_tuned_model, adapter_path)\n",
    "        # Don't create a pipeline for the fine-tuned model\n",
    "    else:\n",
    "        fine_tuned_model = None\n",
    "        tokenizer = None\n",
    "\n",
    "    return base_pipe, (fine_tuned_model, tokenizer)\n",
    "\n",
    "def generate_with_model(model, tokenizer, prompt, max_new_tokens=35000):\n",
    "    \"\"\"Generate response directly using model and tokenizer\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=10,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
    "\n",
    "def generate_response(pipe_or_tuple, prompt):\n",
    "    \"\"\"Generate response using either pipeline or direct model\"\"\"\n",
    "    if isinstance(pipe_or_tuple, tuple):\n",
    "        # For fine-tuned model\n",
    "        model, tokenizer = pipe_or_tuple\n",
    "        return generate_with_model(model, tokenizer, prompt)\n",
    "    else:\n",
    "        # For base model (using pipeline)\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": 4096,\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "            \"top_k\": 10,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "        \n",
    "        if not isinstance(prompt, list):\n",
    "            prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            \n",
    "        output = pipe_or_tuple(prompt, **generation_args)\n",
    "        return output[0]['generated_text']\n",
    "\n",
    "def compare_models(test_prompt, base_model_id, adapter_path):\n",
    "    \"\"\"Compare responses from base and fine-tuned models\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    base_pipe, fine_tuned = load_models(base_model_id, adapter_path)\n",
    "\n",
    "    print(\"\\nGenerating base model response...\")\n",
    "    base_response = generate_response(base_pipe, test_prompt)\n",
    "\n",
    "    print(\"\\nGenerating fine-tuned model response...\")\n",
    "    fine_tuned_response = generate_response(fine_tuned, test_prompt)\n",
    "\n",
    "    print(\"\\n=== Base Model Response ===\")\n",
    "    print(base_response)\n",
    "    print(\"\\n=== Base Model End ===\")\n",
    "    print(\"\\n=== Fine-tuned Model Response ===\")\n",
    "    print(fine_tuned_response)\n",
    "    print(\"\\n=== Fine-tuned Model Response End ===\")\n",
    "    return base_response, fine_tuned_response\n",
    "    \n",
    "base_model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "adapter_path = \"./phi/checkpoint-1\"\n",
    "base_response, fine_tuned_response = compare_models(document_text, base_model_id, adapter_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
